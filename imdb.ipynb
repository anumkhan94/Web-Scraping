import urllib.request, urllib.parse, urllib.error
from bs4 import BeautifulSoup
import ssl
import csv
import requests
import re
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')
from nltk.tokenize import word_tokenize

ctx = ssl.create_default_context()
ctx.check_hostname = False
ctx.verify_mode = ssl.CERT_NONE

headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'}
url = input('Enter - ')
# https://www.imdb.com/chart/top/?ref_=nv_mv_250
page = requests.get(url, headers = headers)
soup = BeautifulSoup(page.content, "html.parser")
container = soup.find_all('td', class_ = 'titleColumn')
# container2 = 
print(len(container))


parentUrl = "https://www.imdb.com"

movie_names = []
movies = container[:50] #here we get the top 50 movies we want
movieData = []
parentUrl = "https://www.imdb.com"
for movie in movies:
    movieDict = {}
    name = movie.find('a')
    mname = name.text
    movieDict["Movie Name"] = mname
    link = name.get('href',None)
    
# finding movie details
    details_url = parentUrl + link
#     print(details_url)
# https://www.imdb.com/title/tt0111161/?pf_rd_m=A2FGELUUNOQJNL&pf_rd_p=e31d89dd-322d-4646-8962-327b42fe94b1&pf_rd_r=C4QE7PC80NHD9AN059ND&pf_rd_s=center-1&pf_rd_t=15506&pf_rd_i=top&ref_=chttp_tt_1

# https://www.imdb.com/title/tt0111161/reviews?ref_=tt_urv

    print("getting movie genre and reviews for "+mname)

    #getting genre for each movie

    genre_page = requests.get(details_url, headers=headers)
    soup = BeautifulSoup(genre_page.content, "html.parser")
    tag = soup.find_all('div', class_ = "see-more inline canwrap") #find_all to return a list
    genre = tag[-1].find('a')
    genreName = genre.text
    print(genreName)

    #getting reviews for each movie
    reviewLink = link.split("?pf_rd")
#     print(reviewLink)
    review_page = requests.get(parentUrl + reviewLink[0]+"reviews?ref_=tt_urv", headers=headers)
    soup = BeautifulSoup(review_page.content, "html.parser")
    divs = soup.find_all('div', class_ = "text show-more__control") #find_all to return a list
#     print(len(divs))
    
    reviewsList = []
    for i in range(len(divs)):
        import re
        s = divs[i].text
        s = re.sub(r'[^\w\s]','',s)
        reviewsList.append(s)
#         print("--------------------")
    movieDict["Genre"] = genreName.lstrip()
    movieDict["Reviews"] = reviewsList
    
    movieData.append(movieDict)
print(movieData[0:2])
    
# saving to csv file

# print(movieData[0], len(movieData))
import csv

csv_file = "imdb.csv"
csv_columns = ["Movie Name", "Genre", "Reviews"]
try:
    with open(csv_file, 'w', encoding="utf-8") as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)
        writer.writeheader()
        count = 1
#         print(finalData)
        for data in movieData:
            writer.writerow(data)
#             print(count)
            count+=1
    print("Data saved to imdb.csv")

except IOError:
    print("I/O error")
    
# storing data in SQLite
import pandas as pd
import sqlite3

conn = sqlite3.connect('imdb.db')
c = conn.cursor()
c.execute('DROP TABLE IF EXISTS movies')
c.execute('''CREATE TABLE movies (movie_name text, genre text, review text)''')


# load the data into a Pandas DataFrame
movies = pd.read_csv('imdb.csv')
# movies = movies.drop(label)

# write the data to a sqlite table
# movies.head(2)

# for row in movies.iterrows:
for index, row in movies.iterrows(): 
#     print (row["Movie Name"]) 
    name = row["Movie Name"]
    genre = row["Genre"]
    review = row["Reviews"]
    
    c.execute('INSERT INTO movies (movie_name, genre, review) VALUES (?,?,?)',
                       (name, genre, review))
    conn.commit()
    print(name, genre)
c.close()
conn.close()
print("Data saved to Imdb.db")

